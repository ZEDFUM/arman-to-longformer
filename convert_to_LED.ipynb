{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install SentencePiece -q"
      ],
      "metadata": {
        "id": "DtbICexkJX_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb87bd88-9de2-4231-ee2a-fb8077b31d14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = \"path/to/save/model\""
      ],
      "metadata": {
        "id": "E6YvKC8NOKvI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Tuple, Dict\n",
        "from torch import nn, Tensor\n",
        "\n",
        "from transformers import  PegasusForConditionalGeneration,LongformerSelfAttention,AutoConfig,PegasusTokenizerFast\n",
        "\n",
        "\n",
        "class LongformerEncoderDecoderForConditionalGeneration(PegasusForConditionalGeneration):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i, layer in enumerate(self.model.encoder.layers):\n",
        "            layer.self_attn = LongformerSelfAttentionForArman(config, layer_id=i)\n",
        "                \n",
        "\n",
        "class LongformerSelfAttentionForArman(LongformerSelfAttention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        output_attentions=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n"
      ],
      "metadata": {
        "id": "7WXTSyNdJNJF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_LpTJ1HIyUj",
        "outputId": "501323ff-b992-4f73-cb0d-8c6b70714a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_encoder_position_embeddings:  8192\n",
            "max_decoder_position_embeddings:  512\n",
            "ModuleList(\n",
            "  (0-11): 12 x PegasusEncoderLayer(\n",
            "    (self_attn): PegasusAttention(\n",
            "      (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "    )\n",
            "    (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (activation_fn): ReLU()\n",
            "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import AutoTokenizer,LongformerSelfAttention\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "\n",
        "def create_long_model(save_model_to, attention_window, max_pos):\n",
        "  \n",
        "    base_model = 'alireza7/ARMAN-MSR-persian-base'\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
        "\n",
        "    tokenizer =  AutoTokenizer.from_pretrained(base_model, model_max_length=max_pos)\n",
        "    \n",
        "    config = model.config\n",
        "\n",
        "    config.attention_probs_dropout_prob = config.attention_dropout\n",
        "    config.architectures = ['LongformerEncoderDecoderForConditionalGeneration', ]\n",
        "\n",
        "    # extend position embeddings\n",
        "    tokenizer.model_max_length = max_pos\n",
        "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
        "    current_max_pos, embed_size = model.model.encoder.embed_positions.weight.shape\n",
        "    # assert current_max_pos == config.max_position_embeddings\n",
        "    config.max_position_embeddings = max_pos\n",
        "    config.max_encoder_position_embeddings = max_pos \n",
        "    config.max_decoder_position_embeddings = 512 #can be different\n",
        "    print(\"max_encoder_position_embeddings: \", config.max_encoder_position_embeddings)\n",
        "    print(\"max_decoder_position_embeddings: \", config.max_decoder_position_embeddings)\n",
        "\n",
        "    # del config.max_position_embeddings \n",
        "\n",
        "    assert max_pos > current_max_pos\n",
        "\n",
        "\n",
        "    # allocate a larger position embedding matrix\n",
        "    new_encoder_pos_embed = model.model.encoder.embed_positions.weight.new_empty(max_pos, embed_size)\n",
        "    # copy position embeddings over and over to initialize the new position embeddings\n",
        "\n",
        "    # k = 0\n",
        "    # step = current_max_pos\n",
        "    k = 0\n",
        "    step = current_max_pos - k\n",
        "    while k < max_pos - 1:\n",
        "        new_encoder_pos_embed[k:(k + step)] = model.model.encoder.embed_positions.weight[:]\n",
        "        k += step\n",
        "\n",
        "    model.model.encoder.embed_positions = torch.nn.Embedding.from_pretrained(new_encoder_pos_embed)\n",
        "\n",
        "    print(model.model.encoder.layers)\n",
        "\n",
        "\n",
        "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
        "    config.attention_dilation = [1] * config.num_hidden_layers\n",
        "\n",
        "    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "    for i, layer in enumerate(model.model.encoder.layers):\n",
        "        longformer_self_attn_for_pegasus = LongformerSelfAttention(config, layer_id=i)\n",
        "\n",
        "        longformer_self_attn_for_pegasus.query = layer.self_attn.q_proj\n",
        "        longformer_self_attn_for_pegasus.key = layer.self_attn.k_proj\n",
        "        longformer_self_attn_for_pegasus.value = layer.self_attn.v_proj\n",
        "\n",
        "        longformer_self_attn_for_pegasus.query_global = copy.deepcopy(layer.self_attn.q_proj)\n",
        "        longformer_self_attn_for_pegasus.key_global = copy.deepcopy(layer.self_attn.k_proj)\n",
        "        longformer_self_attn_for_pegasus.value_global = copy.deepcopy(layer.self_attn.v_proj)\n",
        "\n",
        "        longformer_self_attn_for_pegasus.output = layer.self_attn.out_proj\n",
        "        layer.self_attn = longformer_self_attn_for_pegasus\n",
        "\n",
        "    print(\"OK\")\n",
        "    logger.info(f'saving model to {save_model_to}')\n",
        "    model.save_pretrained(save_model_to)\n",
        "    tokenizer.save_pretrained(save_model_to)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def main():\n",
        "    model, tokenizer = create_long_model(save_model_to=current_directory, attention_window=512, max_pos=8192)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}
