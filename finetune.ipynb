{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODLQ8MUJfmi4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR9A9aaMci2i"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"zedfum/long-summarization-persian\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GRz0rksYb3h"
      },
      "outputs": [],
      "source": [
        "val_dataset = load_dataset(\"zedfum/long-summarization-persian\", split=\"validation[:10%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-gvC8jiYtS_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpUr9QeebZ-n"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"zedfum/arman-longformer-8k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbb24Uh-Y4xO"
      },
      "outputs": [],
      "source": [
        "max_input_length = 8192\n",
        "max_output_length = 512\n",
        "batch_size = 1\n",
        "batch_size_eval=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEcAaZhNY8ge"
      },
      "outputs": [],
      "source": [
        "def process_data_to_model_inputs(batch):\n",
        "    # tokenize the inputs and labels\n",
        "    inputs = tokenizer(\n",
        "        batch[\"article\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "    )\n",
        "    outputs = tokenizer(\n",
        "        batch[\"summary\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_output_length,\n",
        "    )\n",
        "\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    # create 0 global_attention_mask lists\n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    # We have to make sure that the PAD token is ignored\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
        "        for labels in batch[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RClMjZOZCPO"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"summary\", \"Unnamed: 0\",\"id\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy_9DCocZD5G"
      },
      "outputs": [],
      "source": [
        "val_dataset = val_dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size_eval,\n",
        "    remove_columns=[\"article\", \"summary\", \"Unnamed: 0\",\"id\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci2QYHCMZiNO"
      },
      "outputs": [],
      "source": [
        "train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")\n",
        "val_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZf_8QXJacIc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-UfEo0Zadpl"
      },
      "outputs": [],
      "source": [
        "led = AutoModelForSeq2SeqLM.from_pretrained(\"zedfum/arman-longformer-8k\", gradient_checkpointing=True, use_cache=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPnNi_tWaklV"
      },
      "outputs": [],
      "source": [
        "# set generate hyperparameters\n",
        "led.config.num_beams = 2\n",
        "led.config.max_length = 512\n",
        "led.config.min_length = 100\n",
        "led.config.length_penalty = 2.0\n",
        "led.config.early_stopping = True\n",
        "led.config.no_repeat_ngram_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ahmf7mcZzU7"
      },
      "outputs": [],
      "source": [
        "import bert_score\n",
        "from rouge import Rouge\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    P, R, F1 = bert_score.score(pred_str, label_str, lang=\"fa\")\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(pred_str, label_str)\n",
        "\n",
        "    return {\n",
        "        \"bert_precision\": round(P.mean(), 4),\n",
        "        \"bert_recall\": round(R.mean(), 4),\n",
        "        \"bert_fmeasure\": round(F1.mean(), 4),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq7CajIWaUo5"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzxMupNnMmh1"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size_eval,\n",
        "    fp16=True,\n",
        "    output_dir=\"./checks\",\n",
        "    logging_steps=5,\n",
        "    eval_steps=4000,\n",
        "    save_steps=4000,\n",
        "    optim='adafactor',\n",
        "    save_total_limit=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    hub_token=\"hub_token\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"zedfum/arman-longformer-8k-finetuned-ensani\",\n",
        "    hub_strategy=\"checkpoint\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WPyTYO_JfHW"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=led,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4zkCpeQa2NN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "resume_from_checkpoint=False\n",
        "if len(os.listdir(training_args.output_dir))>3:\n",
        "  if os.path.exists(f'{training_args.output_dir}/last-checkpoint'):\n",
        "    resume_from_checkpoint=f'{training_args.output_dir}/last-checkpoint'\n",
        "  else:\n",
        "    resume_from_checkpoint=True\n",
        "\n",
        "# start training\n",
        "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Transformers",
      "language": "python",
      "name": "transformers"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
